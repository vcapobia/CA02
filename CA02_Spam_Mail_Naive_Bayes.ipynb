{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02-Spam_Mail-Naive_Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1rv70f88rIj"
      },
      "source": [
        "Spam Mail-Naive Bayes-CA02\r\n",
        "Victoria Capobianco\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIVFV0_w8pV_"
      },
      "source": [
        "#import necessary packages\r\n",
        "import os #fetching contents from folder\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJmXqtKy78wk",
        "outputId": "fd9e2e30-d86f-42d3-c777-1072a2880fd6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLHY7bGM94v-"
      },
      "source": [
        "#Function 1: Cleaning and Preparing Data\r\n",
        "\r\n",
        "\"\"\"This function builds a Dictionary of most common 3000 words from all the email content. \r\n",
        "First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and \r\n",
        "any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by \r\n",
        "keeping only most common 3000 words in the dictionary. It returns the Dictionary.\"\"\"\r\n",
        "\r\n",
        "def make_Dictionary(root_dir):\r\n",
        "#define function name\r\n",
        "  all_words = [] \r\n",
        "  #empty list to add all words and symbols in the dictionary\r\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)] \r\n",
        "  #join multiple path components in directory to read folders (f) and return a list of the names of the entries in the directory  \r\n",
        "  for mail in emails: #iterate through items in emails list\r\n",
        "    with open(mail) as m: #opens mail and returns it as file object, m \r\n",
        "      for line in m: #iterate through each line in each item\r\n",
        "        words = line.split() #return list of split strings from each line\r\n",
        "        all_words += words #add list of split strings to assigned variable, all_words \r\n",
        "  dictionary = Counter(all_words) #keeps track of how many times each word is added to all_words list\r\n",
        "  list_to_remove = list(dictionary) #returns dictionary as list\r\n",
        "\r\n",
        "\r\n",
        "  for item in list_to_remove: #iterate through each item in list\r\n",
        "    if item.isalpha() == False: #identifies if item is non-alpha-numeric and assigns it False\r\n",
        "      del dictionary[item] #removes non-alpha-numeric items \r\n",
        "    elif len(item) == 1: #identifies if item is has length 1 (single character alpha numeric)\r\n",
        "      del dictionary[item] #removes single character alpha numerics\r\n",
        "  dictionary = dictionary.most_common(3000) #shrinks dictionary by keeping most common 3000 items\r\n",
        "  return dictionary #returns final dictionary \r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncCuOgvM-NH0"
      },
      "source": [
        "#Function 2: Extract Features and Corresponding Label Matrix\r\n",
        "\r\n",
        "\"\"\"This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to \r\n",
        "the number of email files). The function also analyzes the File Names of each email file and decides if it's a \r\n",
        "Spame or not based on the naming convention. Based on this the function also creates the Labelled Data Column. \r\n",
        "This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column.\"\"\"\r\n",
        "\r\n",
        "def extract_features(mail_dir): #define function\r\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\r\n",
        "  #join multiple path components in directory to read files (fi) and return a list of names of entries in directory\r\n",
        "  features_matrix = np.zeros((len(files),3000)) #returns new array with length of 3000 columns and rows equal to number of email files \r\n",
        "  train_labels = np.zeros(len(files)) #returns new array with rows equal to number of email files and with zeros \r\n",
        "  count = 1; #assign naming convention variable \r\n",
        "  docID = 0; #assign naming convention variable\r\n",
        "  for fil in files: #iterate through files\r\n",
        "    with open(fil) as fi: #opens file and returns it as file object, f\r\n",
        "      for i, line in enumerate(fi): #adds counter as function iterates through files\r\n",
        "        if i ==2: #identifies if item is counted 2 times in list\r\n",
        "          words = line.split() #splits string into list, words\r\n",
        "          for word in words: #iterate through items in list, words\r\n",
        "            wordID = 0 #assign naming convention\r\n",
        "            for i, d in enumerate(dictionary): #adds counter as function iterates through dictionary\r\n",
        "              if d[0] == word: #identifies if item is counted 0 times\r\n",
        "                wordID = i #assign naming convention \r\n",
        "                features_matrix[docID,wordID] = words.count(word) #slices array to count words in docID and wordID columns\r\n",
        "      train_labels[docID] = 0; #slice array where docID is equal to 0 \r\n",
        "      filepathTokens = fil.split('/') #splits strings using / marker\r\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1] \r\n",
        "      if lastToken.startswith(\"spmsg\"): #iterate through array to see if it starts with spmsg\r\n",
        "        train_labels[docID] = 1; #extracting training dataset \r\n",
        "        count = count + 1 #creating label column\r\n",
        "      docID = docID + 1 #creating label column\r\n",
        "  return features_matrix, train_labels #return feature dataset and label column\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVzv29q17sUL"
      },
      "source": [
        "#Training and Test Datasets\r\n",
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA_02/Data/train-mails'\r\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA_02/Data/test-mails'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzFO89hk-aHD",
        "outputId": "e6207622-0b52-4e49-f12c-538042fab611"
      },
      "source": [
        "#Main Program \r\n",
        "dictionary = make_Dictionary(TRAIN_DIR)\r\n",
        "\r\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\r\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\r\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\r\n",
        "\r\n",
        "model = GaussianNB()\r\n",
        "\r\n",
        "print (\"Training Model using Gaussian Naive Bayes algorithm .....\")\r\n",
        "model.fit(features_matrix, labels)\r\n",
        "print (\"Training completed\")\r\n",
        "print (\"testing trained model to predict Test Data labels\")\r\n",
        "predicted_labels = model.predict(test_features_matrix)\r\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\r\n",
        "print (accuracy_score(test_labels, predicted_labels))\r\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}